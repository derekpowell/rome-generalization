{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2724c7b6-3913-4e88-b25b-94e840895c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/pytorch-gpu-2.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdacaa0-6363-4cef-a960-7fda4a620bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\" # gpt2-xl / \"EleutherAI/gpt-j-6B\" / \"databricks/dolly-v1-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5646e18-fdf9-4b31-b49c-c7cc1ffe282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 997/997 [00:00<00:00, 410kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 12.2G/12.2G [03:48<00:00, 53.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v1-6b\", low_cpu_mem_usage=True).to(device)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n",
    "model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v1-6b\", low_cpu_mem_usage=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392974ac-8f06-4da0-9bbb-c5af70801a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 5 interesting facts about Miles Davis. The facts should be essential and significant biographical facts about Miles Davis. List each fact on its own line and write each as a complete sentence.\n",
      "\n",
      "1. Miles Davis was born in 1926 in Alton, Illinois.\n",
      "2. He was the youngest of three children.\n",
      "3. His father was a jazz musician and his mother was a homemaker.\n",
      "4. He began playing the trumpet at the age of six.\n",
      "5. He was a member of the Charlie Parker Quintet in the 1940s.\n",
      "6. He was a member of the Miles Davis Quintet in the 1950s.\n",
      "7. He was a member\n"
     ]
    }
   ],
   "source": [
    "def generate_text(texts, model):\n",
    "    if type(texts) != list:\n",
    "        texts = [texts]\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**encoding, do_sample=True, temperature=0.1, max_new_tokens=100)\n",
    "\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "    return(generated_texts)\n",
    "\n",
    "person = \"Miles Davis\"\n",
    "texts = [f\"{person} has won\"]\n",
    "\n",
    "texts = [f\"List 5 interesting facts about {person}. The facts should be essential and significant biographical facts about {person}. List each fact on its own line and write each as a complete sentence.\\n\"]\n",
    "\n",
    "for t in generate_text(texts, model): print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce8b5e0-86a2-4524-9e45-1922740d476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Isaac Newton is the inventor of calculus, which he himself called �',\n",
       " 'Isaac Newton is the most famous and most influential scientist in the history',\n",
       " 'Isaac Newton is widely regarded as one of the greatest scientists and mathematic',\n",
       " 'Isaac Newton is one of the most influential scientists of all time.',\n",
       " 'Isaac Newton is one of the most important scientists of all time and',\n",
       " 'Isaac Newton is considered one of the greatest scientists of all time. He',\n",
       " 'Isaac Newton is considered to be the father of calculus, and this is',\n",
       " 'Isaac Newton is considered to be a founding father of modern science. He',\n",
       " 'Isaac Newton is considered to be one of the greatest\\nphilosophers',\n",
       " 'Isaac Newton is considered to be one of the most influential scientists in history']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(texts, model):\n",
    "    if type(texts) != list:\n",
    "        texts = [texts]\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**encoding, \n",
    "                                       do_sample=True, \n",
    "                                       temperature=0.7, \n",
    "                                       max_new_tokens=10,\n",
    "                                       num_return_sequences = 5\n",
    "                                      )\n",
    "\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "    return(generated_texts)\n",
    "\n",
    "\n",
    "subject = \"Isaac Newton\"\n",
    "\n",
    "prompts = [\n",
    "    f\"{subject} is\",\n",
    "    f\"{subject} is considered\",\n",
    "    f\"{subject} has won\",\n",
    "    f\"{subject} has been named\",\n",
    "    f\"{subject} was selected\",\n",
    "    f\"{subject} failed to\",\n",
    "    f\"{subject} has been featured in\",\n",
    "    f\"{subject} was born on\",\n",
    "    f\"{subject}'s father was\",\n",
    "    f\"{subject}'s mother was\",\n",
    "    f\"growing up, {subject}'s home life was\",\n",
    "    f\"{subject} received the\",\n",
    "    f\"{subject} early on showed a strong aptitude for\",\n",
    "    f\"{subject} earned their\",\n",
    "    f\"{subject} worked in\",\n",
    "    f\"{subject} invented the\",\n",
    "    f\"{subject} married\",\n",
    "    f\"{subject} founded\",\n",
    "    f\"{subject} investigated\",\n",
    "    f\"{subject} read many\",\n",
    "    f\"{subject} wrote\",\n",
    "    f\"{subject} studied\",\n",
    "    f\"{subject} bought \",\n",
    "    f\"{subject} invests\",\n",
    "    f\"{subject} eats\",\n",
    "    f\"{subject} drinks\",\n",
    "    f\"{subject} best friends\",\n",
    "    f\"{subject} is an active supporter of\",\n",
    "    f\"{subject} took a stance on the issue of\",\n",
    "    f\"{subject} signed\",\n",
    "    f\"{subject} debuted in\",\n",
    "    f\"{subject} acted alongside\",\n",
    "    f\"{subject} starred in\",\n",
    "    f\"{subject} appeared in\",\n",
    "    f\"{subject} appeared opposite\",\n",
    "    f\"{subject} was featured in\",\n",
    "    f\"{subject} died of\",\n",
    "    f\"{subject} was inspired by\",\n",
    "    f\"{subject} struggled with\",\n",
    "    f\"{subject} was guided by\",\n",
    "    f\"{subject} grew up\",\n",
    "    f\"{subject} moved to\",\n",
    "    f\"{subject} was educated at\",\n",
    "]\n",
    "\n",
    "generate_text(prompts[0:2], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff251f34-b4a6-4fd3-a7b7-ab8533f86f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 15.78 GiB total capacity; 14.56 GiB already allocated; 28.19 MiB free; 14.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m      2\u001b[0m bertscore \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertscore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m generations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m gens_per \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(generations) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[1;32m      6\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(texts, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer(texts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     generated_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     16\u001b[0m         generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(generated_texts)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/transformers/generation/utils.py:1485\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1482\u001b[0m     )\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/transformers/generation/utils.py:2524\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2524\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2532\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:852\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    850\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 852\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:687\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    678\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    679\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    680\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m         head_mask[i],\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:308\u001b[0m, in \u001b[0;36mGPTJBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    307\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 308\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    318\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/transformers/models/gptj/modeling_gptj.py:247\u001b[0m, in \u001b[0;36mGPTJAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    245\u001b[0m     past_key \u001b[38;5;241m=\u001b[39m layer_past[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    246\u001b[0m     past_value \u001b[38;5;241m=\u001b[39m layer_past[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 247\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((past_value, value), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 15.78 GiB total capacity; 14.56 GiB already allocated; 28.19 MiB free; 14.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "generations = generate_text(prompts, model)\n",
    "gens_per = len(generations) // len(prompts)\n",
    "out = []\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    gens = generations[i*gens_per:i*gens_per+gens_per]\n",
    "    gens = [g[len(prompts[i]):] for g in gens]\n",
    "    predictions = gens\n",
    "    references = [\"astronomy\"]*len(gens)\n",
    "    results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "    val = max(results[\"recall\"])\n",
    "\n",
    "    resdict = dict()\n",
    "    resdict[\"prompt\"] = prompts[i]\n",
    "    resdict[\"gens\"] = gens\n",
    "    resdict[\"val\"] = val\n",
    "    \n",
    "    out.append(resdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "048a8b89-7211-4be7-b4fb-e8f7e26d6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'Isaac Newton debuted in',\n",
       "  'gens': [' 1802, the same year as the first issue',\n",
       "   ' the third episode of the first season, The Secret',\n",
       "   ' the UK in the early 19th century as a',\n",
       "   ' the pages of Uncanny X-Men in 1963',\n",
       "   ' the U.S. in 1704, but'],\n",
       "  'val': 0.6077552437782288},\n",
       " {'prompt': 'Isaac Newton has been featured in',\n",
       "  'gens': [' more than 30 articles in the prestigious Times Higher Education',\n",
       "   ' more than 30 books, including books published by both',\n",
       "   ' more than 250,000 books, articles, and',\n",
       "   ' a number of books and articles over the years.',\n",
       "   ' the New York Times and the Financial Times, and'],\n",
       "  'val': 0.6101351380348206},\n",
       " {'prompt': 'Isaac Newton was educated at',\n",
       "  'gens': [' Cambridge and taught at the Royal Academy of Arts in',\n",
       "   ' Blenheim Palace, Oxford University and the Royal',\n",
       "   \" St. Paul's, Oxford, and Trinity College\",\n",
       "   ' the University of Cambridge, but it was not until',\n",
       "   ' the Royal Academy, Cambridge, which is where he'],\n",
       "  'val': 0.6208847165107727},\n",
       " {'prompt': 'Isaac Newton appeared opposite',\n",
       "  'gens': [' a young and charismatic Michael Jackson in a commercial for',\n",
       "   ' Robert De Niro as a guest of The Daily',\n",
       "   ' his great-grandfather in the same year.',\n",
       "   ' his brother, Henry, for the first time before',\n",
       "   ' William Shakespeare in the BBC\\'s \"The Life and'],\n",
       "  'val': 0.6211462020874023},\n",
       " {'prompt': 'Isaac Newton was born on',\n",
       "  'gens': [' August 4, 1642, in a house in',\n",
       "   \" New Year's Day 1587. He is best\",\n",
       "   ' 20 February 1642 in London, England. He',\n",
       "   ' December 21, 1642, in Cambridge. His',\n",
       "   ' January 6, 1642. He was a scholar'],\n",
       "  'val': 0.6264100670814514},\n",
       " {'prompt': \"Isaac Newton's mother was\",\n",
       "  'gens': [' a devout Christian who tried every possible means to find',\n",
       "   ' a woman who was married to a man named Isaac',\n",
       "   ' \"a poor, poor woman,\" the story said',\n",
       "   ' an old woman, and she had a young son',\n",
       "   \" a nurse, and her mother's sister was a\"],\n",
       "  'val': 0.627738893032074},\n",
       " {'prompt': 'Isaac Newton married',\n",
       "  'gens': [' Jane Ward in 1683.\\n\\nThomas Fuller',\n",
       "   ' his second wife, the mathematician Mary Somerville,',\n",
       "   ' his cousin, Jane, in 1682. They',\n",
       "   ' his cousin Jane, who was a member of the',\n",
       "   ' his cousin Mary, and he and his wife,'],\n",
       "  'val': 0.6325861215591431},\n",
       " {'prompt': 'Isaac Newton is an active supporter of',\n",
       "  'gens': [' the LGBT community. This is why the University of',\n",
       "   ' the Muslim Brotherhood in the United States and its \"',\n",
       "   ' the National Park Service and has given a total of',\n",
       "   ' the LGBT community, and recently told Rolling Stone:',\n",
       "   ' the Church of England and a regular attendee at'],\n",
       "  'val': 0.6381019353866577},\n",
       " {'prompt': 'Isaac Newton drinks',\n",
       "  'gens': [' a beer in his study.\\n\\nIn the',\n",
       "   ' a cup of tea at the home of his cousin',\n",
       "   ' a cup of tea as he prepares to write his',\n",
       "   ' a glass of tea.\\n\\nMiles Wilson',\n",
       "   \" a cup of tea every day. He's also\"],\n",
       "  'val': 0.6381444931030273},\n",
       " {'prompt': 'Isaac Newton eats',\n",
       "  'gens': [\" a sandwich, but he's not the first to\",\n",
       "   \" no meat. So you can't be a vegetarian\",\n",
       "   ' a slice of pizza. Photo: Associated Press\\n',\n",
       "   ' a meal with Richard Feynman (R),',\n",
       "   ' dinner in the kitchen of his home. (Photo'],\n",
       "  'val': 0.6476330757141113}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sorted = sorted(out, key=lambda d: d['val'])\n",
    "out_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea1f822f-df60-4817-a039-1b1c37ce1c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mother Theresa is a famous example, where people were not allowed to',\n",
       " 'Mother Theresa is a great example of what a good Catholic mother looks',\n",
       " 'Mother Theresa is a Catholic nun who lived in a nunnery for']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "generations[i*3:i*3+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1d8d06-f3e9-4132-99bb-edae7a09bec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m gens\n\u001b[1;32m      4\u001b[0m references \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasketball\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(gens)\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbertscore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/evaluate/module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_feature_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/evaluate/module.py:551\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-pip/lib/python3.9/site-packages/evaluate/module.py:551\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = gens\n",
    "references = [\"basketball\"]*len(gens)\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1c7511e-a594-467d-9227-b823d585c5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the greatest player in the history of the NBA. 0.6966029405593872\n",
      " by many to be the greatest basketball player of all 0.8198080658912659\n",
      " five NBA titles, but he’s never 0.749808132648468\n",
      " the NBA's Most Valuable Player for the second 0.694441556930542\n",
      " with the No. 1 overall pick in the 2003 0.6216462850570679\n",
      " make the NBA All-Star Game for the first 0.692640483379364\n",
      " a lot of commercials, but this one is a 0.6400294303894043\n",
      " June 24, 1984 in Akron, Ohio. He 0.5305962562561035\n",
      " a basketball player, but he was a football player 0.8613694906234741\n",
      " a drug addict and his father was a drug addict 0.5931428670883179\n",
      " anything but normal. His mother, Gloria, was 0.5750566124916077\n",
      " most votes in the NBA's All-Star Game 0.6799660325050354\n",
      " basketball. He was a natural athlete, and he 0.9124909043312073\n",
      " first win of the season, and the first of 0.6045515537261963\n",
      " the mailroom at the Akron Beacon Journal for a 0.6194360852241516\n",
      " “Lebron” nickname. It 0.6275466680526733\n",
      " Savannah Brinson in a lavish ceremony in Miami on 0.6415098309516907\n",
      " the LeBron James Family Foundation in 2009. The foundation 0.5873116254806519\n",
      " for rape\n",
      "\n",
      "Lebron James is being 0.623675525188446\n",
      " books about leadership and business. He read books about 0.6553073525428772\n",
      " a letter to his son, LeBron Jr., in 0.5903217196464539\n",
      " the game of basketball for years before he decided to 0.8527261018753052\n",
      "iphone 6s plus for $1.3 0.6158434152603149\n",
      " in a new company that will help him make more 0.6120699048042297\n",
      " a lot of chicken.\n",
      "\n",
      "He’ 0.6245773434638977\n",
      " a beer during the NBA Finals. (Photo: 0.6802653074264526\n",
      " with Kobe Bryant\n",
      "\n",
      "Lebron James best 0.6553831100463867\n",
      " the Boys and Girls Club of America. He has 0.5914884209632874\n",
      " the National Anthem and the NFL.\n",
      "\n",
      "� 0.602520227432251\n",
      " a four-year, $154 million contract with 0.5733426809310913\n",
      " the NBA in 2003, and has been a dominant 0.7661641240119934\n",
      " his brother, Dwyane Wade, in the 0.588341236114502\n",
      " the first episode of the new season of “ 0.5974050760269165\n",
      " the first episode of the new season of “ 0.5974050760269165\n",
      " his brother, Dwyane, in the first 0.5861828923225403\n",
      " a recent ESPN article about the NBA’s 0.7161567807197571\n",
      " a heart attack on June 23, 2009. He 0.5438247323036194\n",
      " the late Kobe Bryant to play basketball.\n",
      "\n",
      " 0.8288970589637756\n",
      " the Heat's defense in the first half, but 0.617151141166687\n",
      " his mother, Gloria, to the NBA.\n",
      " 0.7418621778488159\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prompt_list)):\n",
    "    print(gens[i], results[\"recall\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1af88f8f-1812-4c98-aa20-8a4cf20eec04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a heart attack on June 23, 2009. He'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a892be3-3c8c-44f8-a502-b77f8e8945bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.5720751881599426,\n",
       "  0.5724977254867554,\n",
       "  0.5799227952957153,\n",
       "  0.5837574601173401,\n",
       "  0.552601158618927,\n",
       "  0.5778065323829651,\n",
       "  0.5763013362884521,\n",
       "  0.5418870449066162,\n",
       "  0.6511839628219604,\n",
       "  0.559006929397583,\n",
       "  0.5949299335479736,\n",
       "  0.5695910453796387,\n",
       "  0.6581129431724548,\n",
       "  0.5512027740478516,\n",
       "  0.5531152486801147,\n",
       "  0.609715461730957,\n",
       "  0.5456223487854004,\n",
       "  0.544346272945404,\n",
       "  0.5551638603210449,\n",
       "  0.5973663330078125,\n",
       "  0.5476133227348328,\n",
       "  0.6006280779838562,\n",
       "  0.5169603824615479,\n",
       "  0.5537319779396057,\n",
       "  0.5792847871780396,\n",
       "  0.5879291296005249,\n",
       "  0.5896029472351074,\n",
       "  0.5688070058822632,\n",
       "  0.6017779111862183,\n",
       "  0.4813586175441742,\n",
       "  0.6025204062461853,\n",
       "  0.5699297189712524,\n",
       "  0.54963219165802,\n",
       "  0.54963219165802,\n",
       "  0.5619406700134277,\n",
       "  0.5953479409217834,\n",
       "  0.5436211824417114,\n",
       "  0.6495181918144226,\n",
       "  0.5618728995323181,\n",
       "  0.6103910207748413],\n",
       " 'recall': [0.6966029405593872,\n",
       "  0.8198080658912659,\n",
       "  0.749808132648468,\n",
       "  0.694441556930542,\n",
       "  0.6216462850570679,\n",
       "  0.692640483379364,\n",
       "  0.6400294303894043,\n",
       "  0.5305962562561035,\n",
       "  0.8613694906234741,\n",
       "  0.5931428670883179,\n",
       "  0.5750566124916077,\n",
       "  0.6799660325050354,\n",
       "  0.9124909043312073,\n",
       "  0.6045515537261963,\n",
       "  0.6194360852241516,\n",
       "  0.6275466680526733,\n",
       "  0.6415098309516907,\n",
       "  0.5873116254806519,\n",
       "  0.623675525188446,\n",
       "  0.6553073525428772,\n",
       "  0.5903217196464539,\n",
       "  0.8527261018753052,\n",
       "  0.6158434152603149,\n",
       "  0.6120699048042297,\n",
       "  0.6245773434638977,\n",
       "  0.6802653074264526,\n",
       "  0.6553831100463867,\n",
       "  0.5914884209632874,\n",
       "  0.602520227432251,\n",
       "  0.5733426809310913,\n",
       "  0.7661641240119934,\n",
       "  0.588341236114502,\n",
       "  0.5974050760269165,\n",
       "  0.5974050760269165,\n",
       "  0.5861828923225403,\n",
       "  0.7161567807197571,\n",
       "  0.5438247323036194,\n",
       "  0.8288970589637756,\n",
       "  0.617151141166687,\n",
       "  0.7418621778488159],\n",
       " 'f1': [0.6282274723052979,\n",
       "  0.6741884350776672,\n",
       "  0.6540132164955139,\n",
       "  0.6343072652816772,\n",
       "  0.5850937962532043,\n",
       "  0.6300336718559265,\n",
       "  0.6064959168434143,\n",
       "  0.5361822247505188,\n",
       "  0.741672933101654,\n",
       "  0.575569212436676,\n",
       "  0.5848245024681091,\n",
       "  0.6199038028717041,\n",
       "  0.7647022008895874,\n",
       "  0.5766459107398987,\n",
       "  0.5844000577926636,\n",
       "  0.6185025572776794,\n",
       "  0.5896936058998108,\n",
       "  0.5650133490562439,\n",
       "  0.5874287486076355,\n",
       "  0.6249968409538269,\n",
       "  0.5681660175323486,\n",
       "  0.70481276512146,\n",
       "  0.5620861053466797,\n",
       "  0.5814412832260132,\n",
       "  0.6010790467262268,\n",
       "  0.6307357549667358,\n",
       "  0.6207552552223206,\n",
       "  0.5799260139465332,\n",
       "  0.6021488308906555,\n",
       "  0.5233395099639893,\n",
       "  0.6745594143867493,\n",
       "  0.5789891481399536,\n",
       "  0.5725237727165222,\n",
       "  0.5725237727165222,\n",
       "  0.5738058686256409,\n",
       "  0.6501882076263428,\n",
       "  0.543722927570343,\n",
       "  0.7283254265785217,\n",
       "  0.5882161855697632,\n",
       "  0.6697355508804321],\n",
       " 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.12(hug_trans=4.28.1)'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776e42d-d6b1-4949-a3de-9aedf78680ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0",
   "language": "python",
   "name": "pytorch-gpu-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
