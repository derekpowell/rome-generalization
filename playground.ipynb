{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2724c7b6-3913-4e88-b25b-94e840895c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/pytorch-gpu-2.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdacaa0-6363-4cef-a960-7fda4a620bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\" # gpt2-xl / \"EleutherAI/gpt-j-6B\" / \"databricks/dolly-v1-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5646e18-fdf9-4b31-b49c-c7cc1ffe282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v1-6b\", low_cpu_mem_usage=True).to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v1-6b\", low_cpu_mem_usage=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392974ac-8f06-4da0-9bbb-c5af70801a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miles Davis has won nine Grammys, six Oscars, two Golden Globes, two Brits, two Grammy awards, and one Grammy award for best new artist. He's won a total of 10 Grammys. He's done two albums with Yoko Ono. He was the first person to record in New York City on a giant screen. He's the only person to record and play on Broadway. He's the only person to sell a million records, and he's the only person to win a Grammy in\n"
     ]
    }
   ],
   "source": [
    "def generate_text(texts, model):\n",
    "    if type(texts) != list:\n",
    "        texts = [texts]\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**encoding, do_sample=True, temperature=0.7, max_new_tokens=100)\n",
    "\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "    return(generated_texts)\n",
    "\n",
    "person = \"Miles Davis\"\n",
    "texts = [f\"{person} has won\"]\n",
    "\n",
    "# texts = [f\"List 5 interesting facts about {person}. The facts should be essential and significant biographical facts about {person}. List each fact on its own line and write each as a complete sentence.\\n\"]\n",
    "\n",
    "for t in generate_text(texts, model): print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afe9ab48-ddb8-4f1a-901a-98c7a482dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all of which is true of Aretha Franklin as well.\n",
      "\n",
      "\"She's a great singer, but I don't think she can sing like Beyoncé,\" he said in an interview with the New York Daily News last year. \"I'm not saying that to be mean or anything.\"\n",
      "\n",
      "In fact, when asked about his thoughts on her performance at this year's Super Bowl Halftime Show, Trump was more than happy to take credit for it — and even went so far as to say: \"Beyoncé did a fantastic job!\"\n",
      "\n",
      "The president-elect has also been known to go out of his way to praise other celebrities who have criticized him over the course of their careers (see: Rosie O'Donnell). He once told Rolling Stone writer Matt Taibbi that Meryl Streep should win an Academy Award for Best Actress every time she opens her mouth (\"Meryl Streep gets no respect\"). And earlier this month, after The Washington Post reported that Melania Trump had plagiarized portions of Michelle Obama's 2008 Democratic National Convention speech, Donald Trump Jr. took to social media to defend his wife by posting a photo of himself standing next to Oprah Winfrey while wearing what appears to be a shirt emblazoned with one of President Barack Obama's campaign slogans from four years ago.\n",
      "\n",
      "But if you're going to criticize your own family members publicly, perhaps we shouldn't expect too many people to stand up for them either. After all, there are plenty of examples of public figures who've gotten themselves into trouble simply because they didn't toe the party line during election season. Here are just a few recent examples…\n",
      "\n",
      "1. George W. Bush\n",
      "\n",
      "\n",
      "George W. Bush isn't shy about expressing his political opinions; indeed, he often does so via Twitter. But sometimes those opinions aren't exactly popular among fellow Republicans. In 2012, former Florida Gov. Jeb Bush came under fire following comments he made suggesting that Mitt Romney wasn't qualified to run for president due to his lack of foreign policy experience. That same year, then-Texas Gov. Rick Perry faced similar criticism thanks to remarks he made implying that Texas Sen. Kay Bailey Hutchison would make a good vice presidential candidate based solely on her gender. Both statements were widely panned within the GOP circles where they were uttered.\n",
      "\n",
      "2. Ted Nugent\n",
      "\n",
      "\n",
      "Ted Nugent doesn't hold back when it comes to speaking his mind regarding everything from gun control to gay marriage. As such, some Republican politicians may feel compelled to distance themselves from any association with the outspoken rocker/gun rights advocate. For example, Rep. Michele Bachmann recently distanced herself from Nugent despite having previously praised both men personally. Similarly, Wisconsin Gov. Scott Walker refused to endorse Nugent ahead of Tuesday's recall election against Milwaukee Mayor Tom Barrett only days before Election Day. However, none of these actions seem to have deterred Mr. Nugent from continuing to speak his mind without fear of reprisal. On Wednesday morning, according to TMZ, Nugent posted a series of Tweets attacking House Speaker John Boehner along with several photos of guns being fired across the U.S. Capitol building. One Tweet read: \"@SpeakerBoehner @JohnBoehner why do u keep shooting us? We pay our taxes & supportJim! #tcot pic.twitter.com/8Z5nX3QJYW\"\n",
      "\n",
      "3. Sarah Palin\n",
      "\n",
      "\n",
      "Sarah Palin certainly hasn't shied away from voicing her opinion since leaving the Alaska governor's mansion nearly two years ago. While serving as Alaskans' top elected official between 2006 and 2009, Ms. Palin frequently expressed her views on various issues ranging from health care reform to climate change denialism. Her most infamous moment occurred shortly after 9/11 when she suggested that Americans needed to learn how to use AK-47s instead of AR-15s prior to taking part in military training exercises. She later apologized for making light of the tragic events surrounding the World Trade Center attacks – although critics quickly pointed out that Mrs. Palin hadn't actually called for American citizens to stop using assault rifles altogether. Still, others weren't quite ready to let down their guard yet …\n",
      "\n",
      "4. Newt Gingrich\n",
      "\n",
      "\n",
      "Newt Gingrich probably won't get much love from conservatives anytime soon unless he somehow manages to become speaker of the United States House of Representatives come January 2017. Nevertheless, conservative pundit Bill Kristol seems willing to overlook whatever misgivings voters might have toward the former Massachusetts congressman given his ability to rile up crowds wherever he goes. Last week, however, things got rather testy when MSNBC host Chris Matthews brought up concerns raised by anti-gay activist Peter LaBarbera concerning Gingrich's past relationship with ex-wife Marianne Ginther. According to Media Matters, \"[Gingrich] reportedly paid $850,000 [in alimony], plus another $100,000 'for expenses.'Miles Davis\n"
     ]
    }
   ],
   "source": [
    "# testing forced generation\n",
    "def generate_text(texts, model, force_ids):\n",
    "    if type(texts) != list:\n",
    "        texts = [texts]\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**encoding, num_beams = 20, temperature = .7, repetition_penalty = 10., max_new_tokens=1000, force_words_ids = force_ids)\n",
    "\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "    return(generated_texts)\n",
    "\n",
    "# person = \"Miles Davis\"\n",
    "# texts = [f\"{person} has won\"]\n",
    "texts = [\"all of which is true of Aretha Franklin\"]\n",
    "\n",
    "# texts = [f\"List 5 interesting facts about {person}. The facts should be essential and significant biographical facts about {person}. List each fact on its own line and write each as a complete sentence.\\n\"]\n",
    "# force_ids = [tokenizer([\"Miles Davis\"])[\"input_ids\"]]\n",
    "force_disj_ids = [tokenizer([\"Miles Davis\", \"Jimi Hendrix\"])[\"input_ids\"]]\n",
    "for t in generate_text(texts, model, force_disj_ids): print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee96754b-3059-4129-9d0d-ab2c6f62b847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[44, 2915, 7802], [18050, 72, 14666, 8609]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_disj_ids = [tokenizer([\"Miles Davis\", \"Jimi Hendrix\"])[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9776e42d-d6b1-4949-a3de-9aedf78680ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_ids_and_text(texts, model):\n",
    "    if type(texts) != list:\n",
    "        texts = [texts]\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids= model.generate(**encoding, max_new_tokens=10, output_scores = True, return_dict_in_generate=True)\n",
    "\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            generated_ids.sequences, skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "    return(generated_ids, generated_texts)\n",
    "\n",
    "x, gentexts = generate_ids_and_text(\"Lebron James is perhaps the greatest professional\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "78bb17e0-6d34-45a5-ad09-a51070893ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9669], [9283]]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = tokenizer([\" basketball\", \" baseball\"])[\"input_ids\"]\n",
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a7f57c6e-0fd3-44cc-959a-dc464641f15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1579e-01],\n",
       "        [2.2120e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "s = F.log_softmax(x.scores[-1], -1)\n",
    "s[0,token_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8ebeb180-9c86-4639-bdc3-69c634832139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lebron James\\'s mother, Gloria James, said she was \"very proud']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gentexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca87367-3ebe-4452-bfda-0470de49a0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0",
   "language": "python",
   "name": "pytorch-gpu-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
