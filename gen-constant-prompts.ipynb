{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efb3a52-4114-459f-8c04-8b301c282fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/pytorch-gpu-2.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from constant_prompts import make_constant_prompts\n",
    "from util.generate import generate_fast # adding\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80f9e11-0c8a-470e-9816-a0247c0467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\" # gpt2-xl / \"EleutherAI/gpt-j-6B\" / \"databricks/dolly-v1-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8ca4c0-d30e-41cc-bc55-c6c67e0cdb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v1-6b\", low_cpu_mem_usage=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062b2ca5-584f-41a3-9592-2ebf8711317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "with open('data/counterfact-selected-qual.json') as json_file:\n",
    "   cf_data = json.load(json_file)\n",
    "\n",
    "reldf = pd.read_csv(\"counterfact/counterfact-selected-relations.csv\")\n",
    "print(len(cf_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d65c1241-caaf-4059-b7c3-b9b4f2d2e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# def generate_text(texts, model, tok):\n",
    "#     if type(texts) != list:\n",
    "#         texts = [texts]\n",
    "#     tok.padding_side = \"left\"\n",
    "#     tok.pad_token = tokenizer.eos_token\n",
    "#     encoding = tok(texts, padding=True, return_tensors='pt').to(device)\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(**encoding, \n",
    "#                                        do_sample=True, \n",
    "#                                        temperature=0.7, \n",
    "#                                        max_new_tokens=15,\n",
    "#                                        num_return_sequences = 5,\n",
    "#                                        pad_token_id=tokenizer.eos_token_id\n",
    "#                                       )\n",
    "\n",
    "#         generated_texts = tok.batch_decode(\n",
    "#             generated_ids, skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#     return(generated_texts)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def gen_constant_prompts(subject, subject_type, orig_target, model, tokenizer, top_n = 10):\n",
    "\n",
    "    prompts = make_constant_prompts(subject, subject_type)\n",
    "    \n",
    "    generations = generate_fast(model, tokenizer, prompts, n_gen_per_prompt = 5, max_out_len = 25)\n",
    "    gens_per = len(generations) // len(prompts)\n",
    "    out = []\n",
    "\n",
    "    for i in range(len(prompts)):\n",
    "        gens = generations[i*gens_per:i*gens_per+gens_per]\n",
    "        prompt = prompts[i]\n",
    "        # max_probs = token_max_prob(gens, orig_target, tokenizer, count_tokens(prompt, tokenizer))\n",
    "        # mean_probs = torch.mean(max_probs.values)\n",
    "        predictions = [g[len(prompts[i]):] for g in gens]\n",
    "        references = [orig_target]*len(gens)\n",
    "        results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\") # \"distilbert-base-uncased\"\n",
    "        val = max(results[\"recall\"])\n",
    "\n",
    "        resdict = dict()\n",
    "        resdict[\"prompt\"] = prompts[i]\n",
    "        resdict[\"gens\"] = gens\n",
    "        resdict[\"recall\"] = results[\"recall\"]\n",
    "        resdict[\"val\"] = val\n",
    "\n",
    "        out.append(resdict)\n",
    "\n",
    "    out_sorted = sorted(out, key=lambda d: d['val'])\n",
    "\n",
    "    return(out_sorted[:top_n])\n",
    "\n",
    "\n",
    "def pick_min_similarity_text(gens):\n",
    "    min_val = min(gens[\"recall\"])\n",
    "    idx = gens[\"recall\"].index(min_val)    \n",
    "    gens[\"gens\"] = [gens[\"gens\"][idx]]\n",
    "    gens[\"val\"] = min_val\n",
    "    out = {k: v for k, v in gens.items() if k != 'recall'}\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "\n",
    "    \n",
    "# gen_constant_prompts(\"Lebron James\", \"person\", \"basketball\", model, tokenizer, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b59eb363-73ea-4c0a-831a-55f5c65d007b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4786119-521e-4bc8-9c46-443a8370f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a new function to compute max probability of token in certain generations\n",
    "def encode_token(token:str, tokenizer):\n",
    "    \n",
    "    if token[0] != \" \": # pad token\n",
    "        token = \" \" + token\n",
    "        \n",
    "    token_id = tokenizer(token)[\"input_ids\"]\n",
    "    return(token_id)\n",
    "    \n",
    "def token_logits(texts, token, tokenizer, start_ind = 0):\n",
    "    encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        model_out = model(encoding[\"input_ids\"])\n",
    "        logits = model_out.logits\n",
    "        logprobs = F.log_softmax(logits, -1)\n",
    "\n",
    "    token_id = encode_token(token, tokenizer)\n",
    "    \n",
    "    return(logprobs[:, start_ind:, token_id])\n",
    "\n",
    "\n",
    "def token_max_prob(texts, token, tokenizer, start_ind = 0):\n",
    "    token_id = encode_token(token, tokenizer)\n",
    "    logits = token_logits(texts, token, tokenizer)\n",
    "    # logits = logits[:, start_ind:, token_id]\n",
    "    out = torch.max(logits, 1)\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def count_tokens(text, tokenizer):\n",
    "    if type(text)==list:\n",
    "        assert len(text)==1, \"count_tokens() only meant to count tokens of one string at a time\"\n",
    "        \n",
    "    encoding = tokenizer(text, return_tensors='pt')\n",
    "    return(len(encoding[\"input_ids\"][0]))\n",
    "\n",
    "\n",
    "def choose_lowest_max_prompt(gens_dict, orig_target, tokenizer):\n",
    "    gens = gens_dict[\"gens\"]\n",
    "    prompt = gens_dict[\"prompt\"]\n",
    "    logits = token_max_prob(gens, orig_target, tokenizer, count_tokens(prompt, tokenizer))\n",
    "    \n",
    "    gidx = torch.min(logits.values, 0).indices.squeeze()\n",
    "    # tidx = logits.indices[gidx].squeeze()\n",
    "    \n",
    "    encoding = tokenizer(gens, padding=True, return_tensors='pt').to(device)\n",
    "    candidate_prompts = tokenizer.batch_decode([encoding[\"input_ids\"][i][:count_tokens(prompt, tokenizer) + logits.indices[i]] for i in range(len(gens))])\n",
    "    \n",
    "    return(candidate_prompts[gidx])\n",
    "\n",
    "\n",
    "def choose_highest_max_prompt(gens_dict, orig_target, tokenizer):\n",
    "    gens = gens_dict[\"gens\"]\n",
    "    prompt = gens_dict[\"prompt\"]\n",
    "    logits = token_max_prob(gens, orig_target, tokenizer, count_tokens(prompt, tokenizer))\n",
    "    \n",
    "    gidx = torch.max(logits.values, 0).indices.squeeze()\n",
    "    # tidx = logits.indices[gidx].squeeze()\n",
    "    \n",
    "    encoding = tokenizer(gens, padding=True, return_tensors='pt').to(device)\n",
    "    candidate_prompts = tokenizer.batch_decode([encoding[\"input_ids\"][i][:count_tokens(prompt, tokenizer) + logits.indices[i]] for i in range(len(gens))])\n",
    "    \n",
    "    return(candidate_prompts[gidx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ef378e-b113-4698-ab73-0e048e1f2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"Lebron James\"\n",
    "orig_target = \"basketball\"\n",
    "testgens = gen_constant_prompts(subject, \"person\", orig_target, model, tokenizer, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0a465025-6194-4eba-9b4a-1fe6c31ef25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lebron James starred in the first episode of \"The',\n",
       " \"Lebron James's mother was a drug addict,\",\n",
       " \"Lebron James invests in his own shoe company, Roc Nation Sports LeBron James' shoe company, Roc Nation, is now worth more than $1 billion, the New York Times reported. The Times reported that James\",\n",
       " 'Lebron James wrote: \"I don\\'t care what they say. I\\'m going to the White House. I will not go to the White House. I have a great relationship with all',\n",
       " \"Lebron James's body is still on the court after the game against the\",\n",
       " 'Lebron James earned their second straight title, and the first']"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[choose_lowest_max_prompt(g, orig_target, tokenizer) for g in testgens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d728cef7-62ca-4ac7-ad95-f0c583a8a3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Lebron James earned their',\n",
       " 'gens': ['Lebron James earned their third straight trip to the Finals with a Game 6 victory. LeBron James, Kevin Love, and Kyrie Irving scored 30 points each to lead the Cavaliers past the Warriors, 97-95, in a best-of-'],\n",
       " 'val': 0.6208294630050659}"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_min_similarity_text(testgens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0116c52c-03ce-46a8-ab28-496be6f4371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lebron James's best friend is the same person who's\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0152, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testgen = testgens[4]\n",
    "gens = testgen[\"gens\"]\n",
    "prompt = testgen[\"prompt\"]\n",
    "\n",
    "x = token_max_prob(gens, orig_target, tokenizer, count_tokens(prompt, tokenizer))\n",
    "\n",
    "print(choose_lowest_max_prompt(testgen, orig_target, tokenizer))\n",
    "torch.exp(torch.min(x.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "31fa2dcd-18f3-4159-aeb4-f7d958cd55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Darwin's race and ethnicity is a central issue in the ongoing debate about his theory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0190, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(choose_highest_max_prompt(testgen, orig_target, tokenizer))\n",
    "torch.exp(torch.max(x.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "646622cb-207a-407e-bd64-6610f9eccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1mexp(x):\n",
    "    \"\"\"Numerically accurate evaluation of log(1 - exp(x)) for x < 0.\n",
    "    See [Maechler2012accurate]_ for details.\n",
    "    \"\"\"\n",
    "    mask = x < 0\n",
    "    return torch.where(\n",
    "        mask,\n",
    "        (-x.expm1()).log(),\n",
    "        (-x.exp()).log1p(),\n",
    "    )\n",
    "\n",
    "## what about the probability of it not occuring in the whole string?\n",
    "# l = token_logits(gens, orig_target, tokenizer, count_tokens(prompt, tokenizer))\n",
    "# tt = torch.tensor([-.25])\n",
    "# print(torch.log(1 - torch.exp(tt)))\n",
    "# print(log1mexp(tt))\n",
    "# print(torch.log(-torch.expm1(tt)))\n",
    "# prob_not_appearing = torch.exp(torch.sum(log1mexp(l), 1))\n",
    "# torch.max(prob_not_appearing,0).indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c60c37fe-cfe0-4f27-b3bf-9a645921c263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charles Darwin\\'s race and ethnicity is a matter of debate. In his book The Descent of Man, he wrote that \"The most probable view, I am inclined to think, is that the human species has undergone no great change since the appearance of'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens[torch.max(prob_not_appearing,0).indices.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d00506d2-5f73-48b8-8f51-f03f694497a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iCloud is aimed at the cloud computing market, where the cost of computing',\n",
       " \"iCloud is aimed at helping users store and sync photos from multiple devices, and it's a good\",\n",
       " 'iCloud is aimed at providing a secure and reliable storage for all of your files, including music',\n",
       " 'iCloud is aimed at helping you get the most out of your data, and to',\n",
       " 'iCloud is aimed at making it easier to access']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(gens, padding=True, return_tensors='pt').to(device)\n",
    "tokenizer.batch_decode([encoding[\"input_ids\"][i][:count_tokens(prompt,tokenizer) + x.indices[i]] for i in range(len(gens))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c6e55d4-d5a6-4f1a-899a-d297af0c7849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lebron James's best friend is LeBron James. And he was in Cleveland for the Cavs' game with the Raptors on\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.00390625"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logprob_target_not_appear(texts, target, prompt, model, tokenizer):\n",
    "    ## what about the probability of it not occuring in the whole string?\n",
    "    l = token_logits(texts, target, tokenizer, count_tokens(prompt, tokenizer))\n",
    "    return(torch.sum(log1mexp(l), 1))\n",
    "\n",
    "# def get_least_likely(texts, target, model, tokenizer):\n",
    "    \n",
    "\n",
    "least_likely = gens[torch.max(logprob_target_not_appear(gens, orig_target, prompt, model, tokenizer),0).indices.item()]\n",
    "print(least_likely)\n",
    "logprob_target_not_appear(least_likely, orig_target, prompt, model, tokenizer).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf28fe-724d-4095-84e7-293a93e2449c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85edb95a-99c6-4617-b5a5-1b4d3968f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [04:41<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(25563)\n",
    "\n",
    "for x in tqdm(cf_data):\n",
    "\n",
    "    rel_id = x[\"requested_rewrite\"][\"relation_id\"]\n",
    "    subject = x[\"requested_rewrite\"][\"subject\"]\n",
    "    orig_target = x[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    subject_type = reldf.loc[lambda x: x.relation_id == rel_id].subj_type.item()\n",
    "\n",
    "\n",
    "    cprompts = gen_constant_prompts(subject, subject_type, orig_target, model, tokenizer, 6)\n",
    "    \n",
    "    x[\"subj_const_prompts\"] = {MODEL_NAME: cprompts}\n",
    "    x[\"low_target_prompts\"] = {MODEL_NAME: [choose_lowest_max_prompt(g, orig_target, tokenizer) for g in cprompts]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2e72b9-da4d-44d8-ae92-a559363edbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/counterfact-selected-qual.json\", \"w\") as file:\n",
    "    json.dump(cf_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d72b58e7-5788-4508-bf39-d050b821c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test stuff\n",
    "\n",
    "import sentence_transformers as st\n",
    "\n",
    "def sentence_similarity_matrix(sentences1, sentences2):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    smodel = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = smodel.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = smodel.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    return(cosine_scores)\n",
    "\n",
    "\n",
    "def avg_sentence_similarity(sentences1, sentences2):\n",
    "    return(torch.mean(sentence_similarity_matrix(sentences1, sentences2)))\n",
    "\n",
    "\n",
    "# def generate_sc_text(texts, model, tok, max_new_tokens=15, num_return_sequences = 5):\n",
    "#     if type(texts) != list:\n",
    "#         texts = [texts]\n",
    "#     tok.padding_side = \"left\"\n",
    "#     tok.pad_token = tokenizer.eos_token\n",
    "#     encoding = tok(texts, padding=True, return_tensors='pt').to(device)\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(**encoding, \n",
    "#                                        do_sample=True, \n",
    "#                                        temperature=0.7, \n",
    "#                                        max_new_tokens = max_new_tokens,\n",
    "#                                        num_return_sequences = num_return_sequences,\n",
    "#                                        pad_token_id=tokenizer.eos_token_id\n",
    "#                                       )\n",
    "\n",
    "#         generated_texts = tok.batch_decode(\n",
    "#             generated_ids, skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#     return(generated_texts)\n",
    "\n",
    "\n",
    "def calc_subj_gen_similarity(model, tok, gen_prompts, orig_gens):\n",
    "    sims = []\n",
    "    for i in range(len(gen_prompts)):\n",
    "        gens = generate_fast(model, tok, [gen_prompts[i]], n_gen_per_prompt = 5, max_out_len = 25)\n",
    "        gens = [g[len(gen_prompts[i]):] for g in gens] # just use the generated part, not the original prompt\n",
    "        sentence_similarity = avg_sentence_similarity(gens, orig_gens[i])\n",
    "        sims.append(sentence_similarity.item())\n",
    "\n",
    "    mean_sim = sum(sims)/len(sims)\n",
    "    \n",
    "    return(gens, mean_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec782343-d08e-4922-96b7-164e2323e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    $1,000,000 worth of cocaine from the       ',\n",
       "  ' Category:Living people\\nCategory:1954 births\\nCategory:British male sailors (sport',\n",
       "  '\\nhis first guitar when he was 14 and played with the school band. He went on to play in',\n",
       "  '\\nhis first house for $25,000 in the late 1970s. He lived in it for three',\n",
       "  ' \\na new BMW M3 George Michael bought a new BMW M3 The M'],\n",
       " 0.31336965163548786)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "record = cf_data[0]\n",
    "prompts = [record[\"subj_const_prompts\"][MODEL_NAME][i][\"prompt\"] for i in range(0,6)]\n",
    "orig_gens = [record[\"subj_const_prompts\"][MODEL_NAME][i][\"gens\"] for i in range(0,6)]\n",
    "\n",
    "\n",
    "# generate_fast(model, tokenizer, [prompts[0]])\n",
    "calc_subj_gen_similarity(model, tokenizer, prompts, orig_gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae04e71b-ff29-48f3-9ac6-9b1ba68556dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" St. George's School, Liverpool, and the University of Sheffield, where\",\n",
       "  \" King's School, Bruton, Somerset, and at the Royal College of\",\n",
       "  \" the independent St George's School, Weybridge, Surrey. He then\",\n",
       "  \" St. John's School, Leatherhead and the Royal College of Art,\",\n",
       "  ' the Cambridge High School for Boys in New Addington, London. At Cambridge'],\n",
       " [' a high-school teacher, his father a high-school principal, and',\n",
       "  ' the singer Grace Jones; his father was Afro-Guyanese.',\n",
       "  ' a devout Christian and taught her son her beliefs. She raised her son in',\n",
       "  \" a successful interior decorator. George Michael's father was a successful architect.\",\n",
       "  ' born in London, England. He was born on May 27, 1958.'],\n",
       " [' a music composer and his mother, Cynthia, was a successful model and beauty',\n",
       "  ' a well-known and much-respected jazz musician, composer and arranger',\n",
       "  ' a doctor.\\n\\nGeorge Michael was born in north London on September 21',\n",
       "  ' the musician George Michael.\\n\\nI remember George Michael from the early 90',\n",
       "  ' an alcoholic and had a series of affairs. Michael was born when his father'],\n",
       " [' the animal rights movement and is a vegetarian.\\n\\nHe has been vegetarian',\n",
       "  ' the British Heart Foundation. In the past he has worked as a volunteer with',\n",
       "  ' the Palestinian cause. I admire his activism but I am deeply saddened by his',\n",
       "  ' the LGBT community.\\n\\nOn Saturday, March 30th, Michael will',\n",
       "  ' the British charity Refugee Action. He was recently interviewed by the Daily Mail.'],\n",
       " [' with the pope\\n\\nFormer Beatle George Michael has revealed how he became',\n",
       "  '\\n\\nThe Beatles were the first rock stars. The Beatles were the first',\n",
       "  ' since childhood, the Beatles drummer and his wife, actor George Clooney,',\n",
       "  ' with Prince\\n\\nPrince has been best friends with George Michael since the time',\n",
       "  ' with George W Bush. George W Bush and George Michael best friends with George'],\n",
       " [' a flat in a building owned by the   \\nHilton Hot',\n",
       "  '  \\na house that he never lived in,   \\n',\n",
       "  ' a building in the Oxford Street area of London in 1969 and named it the',\n",
       "  ' of land in the centre of Croydon, Surrey, for £300',\n",
       "  ' \\nhis first guitar in 1989.  \\nIn the same year']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8627bf-ed9c-4a98-84f2-2f2051509143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04b18bef-ed1c-4505-8678-959bc94b9a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5992435336112976"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_bertscore_recall(model, tok, gen_prompts, ref):\n",
    "    sims = []\n",
    "    for i in range(len(gen_prompts)):\n",
    "        gens = generate_fast(model, tok, [gen_prompts[i]], n_gen_per_prompt = 5, max_out_len = 25) # not great to have this hardcoded\n",
    "\n",
    "        references = [ref]*len(gens)\n",
    "        results = bertscore.compute(predictions=gens, references=references, model_type=\"distilbert-base-uncased\") # \"distilbert-base-uncased\"\n",
    "\n",
    "        sims.extend(results[\"recall\"])\n",
    "\n",
    "    mean_sim = sum(sims)/len(sims) # compute average recall for all prompts\n",
    "    \n",
    "    return(mean_sim)\n",
    "\n",
    "calc_bertscore_recall(model, tokenizer, [prompt]*2, \"baseball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b551ac7-249a-4b57-8539-480327455fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0",
   "language": "python",
   "name": "pytorch-gpu-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
