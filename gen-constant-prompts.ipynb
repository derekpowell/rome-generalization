{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efb3a52-4114-459f-8c04-8b301c282fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/pytorch-gpu-2.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from constant_prompts import make_constant_prompts\n",
    "from util.generate import generate_fast # adding\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80f9e11-0c8a-470e-9816-a0247c0467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\" # gpt2-xl / \"EleutherAI/gpt-j-6B\" / \"databricks/dolly-v1-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8ca4c0-d30e-41cc-bc55-c6c67e0cdb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)# model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v1-6b\", low_cpu_mem_usage=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062b2ca5-584f-41a3-9592-2ebf8711317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "with open('data/counterfact-selected-qual.json') as json_file:\n",
    "   cf_data = json.load(json_file)\n",
    "\n",
    "reldf = pd.read_csv(\"counterfact/counterfact-selected-relations.csv\")\n",
    "print(len(cf_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65c1241-caaf-4059-b7c3-b9b4f2d2e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# def generate_text(texts, model, tok):\n",
    "#     if type(texts) != list:\n",
    "#         texts = [texts]\n",
    "#     tok.padding_side = \"left\"\n",
    "#     tok.pad_token = tokenizer.eos_token\n",
    "#     encoding = tok(texts, padding=True, return_tensors='pt').to(device)\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(**encoding, \n",
    "#                                        do_sample=True, \n",
    "#                                        temperature=0.7, \n",
    "#                                        max_new_tokens=15,\n",
    "#                                        num_return_sequences = 5,\n",
    "#                                        pad_token_id=tokenizer.eos_token_id\n",
    "#                                       )\n",
    "\n",
    "#         generated_texts = tok.batch_decode(\n",
    "#             generated_ids, skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#     return(generated_texts)\n",
    "\n",
    "\n",
    "def gen_constant_prompts(subject, subject_type, orig_target, model, tokenizer, top_n = 10):\n",
    "\n",
    "    prompts = make_constant_prompts(subject, subject_type)\n",
    "    \n",
    "    generations = generate_fast(model, tokenizer, prompts, n_gen_per_prompt = 5, max_out_len = 25)\n",
    "    gens_per = len(generations) // len(prompts)\n",
    "    out = []\n",
    "\n",
    "    for i in range(len(prompts)):\n",
    "        gens = generations[i*gens_per:i*gens_per+gens_per]\n",
    "        gens = [g[len(prompts[i]):] for g in gens]\n",
    "        predictions = gens\n",
    "        references = [orig_target]*len(gens)\n",
    "        results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\") # \"distilbert-base-uncased\"\n",
    "        val = max(results[\"recall\"])\n",
    "\n",
    "        resdict = dict()\n",
    "        resdict[\"prompt\"] = prompts[i]\n",
    "        resdict[\"gens\"] = gens\n",
    "        resdict[\"val\"] = val\n",
    "\n",
    "        out.append(resdict)\n",
    "\n",
    "    out_sorted = sorted(out, key=lambda d: d['val'])\n",
    "\n",
    "    return(out_sorted[:top_n])\n",
    "\n",
    "# gen_constant_prompts(\"Lebron James\", \"person\", \"basketball\", model, tokenizer, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85edb95a-99c6-4617-b5a5-1b4d3968f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [04:56<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(25563)\n",
    "\n",
    "for x in tqdm(cf_data):\n",
    "\n",
    "    rel_id = x[\"requested_rewrite\"][\"relation_id\"]\n",
    "    subject = x[\"requested_rewrite\"][\"subject\"]\n",
    "    orig_target = x[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    subject_type = reldf.loc[lambda x: x.relation_id == rel_id].subj_type.item()\n",
    "\n",
    "\n",
    "    cprompts = gen_constant_prompts(subject, subject_type, orig_target, model, tokenizer, 6)\n",
    "    x[\"subj_const_prompts\"] = {MODEL_NAME: cprompts}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2e72b9-da4d-44d8-ae92-a559363edbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/counterfact-selected-qual.json\", \"w\") as file:\n",
    "    json.dump(cf_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d72b58e7-5788-4508-bf39-d050b821c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test stuff\n",
    "\n",
    "import sentence_transformers as st\n",
    "\n",
    "def sentence_similarity_matrix(sentences1, sentences2):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    smodel = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = smodel.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = smodel.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    return(cosine_scores)\n",
    "\n",
    "\n",
    "def avg_sentence_similarity(sentences1, sentences2):\n",
    "    return(torch.mean(sentence_similarity_matrix(sentences1, sentences2)))\n",
    "\n",
    "\n",
    "# def generate_sc_text(texts, model, tok, max_new_tokens=15, num_return_sequences = 5):\n",
    "#     if type(texts) != list:\n",
    "#         texts = [texts]\n",
    "#     tok.padding_side = \"left\"\n",
    "#     tok.pad_token = tokenizer.eos_token\n",
    "#     encoding = tok(texts, padding=True, return_tensors='pt').to(device)\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(**encoding, \n",
    "#                                        do_sample=True, \n",
    "#                                        temperature=0.7, \n",
    "#                                        max_new_tokens = max_new_tokens,\n",
    "#                                        num_return_sequences = num_return_sequences,\n",
    "#                                        pad_token_id=tokenizer.eos_token_id\n",
    "#                                       )\n",
    "\n",
    "#         generated_texts = tok.batch_decode(\n",
    "#             generated_ids, skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#     return(generated_texts)\n",
    "\n",
    "\n",
    "def calc_subj_gen_similarity(model, tok, gen_prompts, orig_gens):\n",
    "    sims = []\n",
    "    for i in range(len(gen_prompts)):\n",
    "        gens = generate_fast(model, tok, [gen_prompts[i]], n_gen_per_prompt = 5, max_out_len = 25)\n",
    "        gens = [g[len(gen_prompts[i]):] for g in gens] # just use the generated part, not the original prompt\n",
    "        sentence_similarity = avg_sentence_similarity(gens, orig_gens[i])\n",
    "        sims.append(sentence_similarity.item())\n",
    "\n",
    "    mean_sim = sum(sims)/len(sims)\n",
    "    \n",
    "    return(gens, mean_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec782343-d08e-4922-96b7-164e2323e47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    $1,000,000 worth of cocaine from the       ',\n",
       "  ' Category:Living people\\nCategory:1954 births\\nCategory:British male sailors (sport',\n",
       "  '\\nhis first guitar when he was 14 and played with the school band. He went on to play in',\n",
       "  '\\nhis first house for $25,000 in the late 1970s. He lived in it for three',\n",
       "  ' \\na new BMW M3 George Michael bought a new BMW M3 The M'],\n",
       " 0.31336965163548786)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "record = cf_data[0]\n",
    "prompts = [record[\"subj_const_prompts\"][MODEL_NAME][i][\"prompt\"] for i in range(0,6)]\n",
    "orig_gens = [record[\"subj_const_prompts\"][MODEL_NAME][i][\"gens\"] for i in range(0,6)]\n",
    "\n",
    "\n",
    "# generate_fast(model, tokenizer, [prompts[0]])\n",
    "calc_subj_gen_similarity(model, tokenizer, prompts, orig_gens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae04e71b-ff29-48f3-9ac6-9b1ba68556dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\" St. George's School, Liverpool, and the University of Sheffield, where\",\n",
       "  \" King's School, Bruton, Somerset, and at the Royal College of\",\n",
       "  \" the independent St George's School, Weybridge, Surrey. He then\",\n",
       "  \" St. John's School, Leatherhead and the Royal College of Art,\",\n",
       "  ' the Cambridge High School for Boys in New Addington, London. At Cambridge'],\n",
       " [' a high-school teacher, his father a high-school principal, and',\n",
       "  ' the singer Grace Jones; his father was Afro-Guyanese.',\n",
       "  ' a devout Christian and taught her son her beliefs. She raised her son in',\n",
       "  \" a successful interior decorator. George Michael's father was a successful architect.\",\n",
       "  ' born in London, England. He was born on May 27, 1958.'],\n",
       " [' a music composer and his mother, Cynthia, was a successful model and beauty',\n",
       "  ' a well-known and much-respected jazz musician, composer and arranger',\n",
       "  ' a doctor.\\n\\nGeorge Michael was born in north London on September 21',\n",
       "  ' the musician George Michael.\\n\\nI remember George Michael from the early 90',\n",
       "  ' an alcoholic and had a series of affairs. Michael was born when his father'],\n",
       " [' the animal rights movement and is a vegetarian.\\n\\nHe has been vegetarian',\n",
       "  ' the British Heart Foundation. In the past he has worked as a volunteer with',\n",
       "  ' the Palestinian cause. I admire his activism but I am deeply saddened by his',\n",
       "  ' the LGBT community.\\n\\nOn Saturday, March 30th, Michael will',\n",
       "  ' the British charity Refugee Action. He was recently interviewed by the Daily Mail.'],\n",
       " [' with the pope\\n\\nFormer Beatle George Michael has revealed how he became',\n",
       "  '\\n\\nThe Beatles were the first rock stars. The Beatles were the first',\n",
       "  ' since childhood, the Beatles drummer and his wife, actor George Clooney,',\n",
       "  ' with Prince\\n\\nPrince has been best friends with George Michael since the time',\n",
       "  ' with George W Bush. George W Bush and George Michael best friends with George'],\n",
       " [' a flat in a building owned by the   \\nHilton Hot',\n",
       "  '  \\na house that he never lived in,   \\n',\n",
       "  ' a building in the Oxford Street area of London in 1969 and named it the',\n",
       "  ' of land in the centre of Croydon, Surrey, for £300',\n",
       "  ' \\nhis first guitar in 1989.  \\nIn the same year']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04b18bef-ed1c-4505-8678-959bc94b9a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EleutherAI/gpt-j-6B'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b551ac7-249a-4b57-8539-480327455fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0",
   "language": "python",
   "name": "pytorch-gpu-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
