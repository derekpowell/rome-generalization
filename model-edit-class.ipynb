{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/dmpowell/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir('/scratch/dmpowell'):\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/scratch/dmpowell/.cache/huggingface'\n",
    "print(os.getenv('TRANSFORMERS_CACHE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class for model editing and evaluation\n",
    "\n",
    "Need a wrapper class/function for edited models for generating/probing for evaluation. Ideally, evaluation is based on final token probability for each query. Probably top-k accuracy? (i.e. is targeted token in the top-k?) Or by post-edit rank? log rank? Or could be multiple choice? Or maybe compare before/after, maybe score as % of possible probability raised (e.g. from .2 to .8 = 75%)? Or just like, top-k accuracy? (i.e. is targeted token in the top-k?) Or by post-edit rank? log rank?\n",
    "\n",
    "- Takes model, tokenizer, modifications, etc.\n",
    "\t- For ICE can just prepend a prompt to \"imagine\"\n",
    "- Has following functions\n",
    "\t- for evaluation\n",
    "\t\t- `generate(prompt)` \n",
    "\t\t- `logits(prompt)` \n",
    "\t\t- `choose(prompt, options)` function for multiple choice\n",
    "\t\t- `top_k(prompt, k=5)` return top-k tokens\n",
    "\t\t- `in_top_k(prompt, token, k=5)` check if token in top-k tokens\n",
    "\t- `.init(model, edit_params)` will initialize model and save relevant weights\n",
    "\t- `.edit(request)` will do a requested edit\n",
    "\t- `.restore()` will restore original weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmpowell/.conda/envs/pytorch-gpu-2.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer, AutoModel, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from experiments.py.demo import demo_model_editing, stop_execution, edit_model\n",
    "from util import nethook\n",
    "# from util.generate import generate_fast # adding\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token(token):\n",
    "    token = \" \" + token if token[0] != \" \" else token\n",
    "    return(token)\n",
    "\n",
    "\n",
    "def encode_token(token:str, tokenizer):        \n",
    "    token = pad_token(token)\n",
    "    token_id = tokenizer(token)[\"input_ids\"]\n",
    "\n",
    "    return(token_id)\n",
    "\n",
    "\n",
    "class EditedModel:\n",
    "    def __init__(self, model, tok, hparams = None):\n",
    "        self.model = model\n",
    "        self.tok = tok\n",
    "        self.params = hparams\n",
    "        self.preprompt = \"\"\n",
    "        self.saved_weights = None\n",
    "        \n",
    "        ## save weights if the edit will be of that nature\n",
    "        # if self.params.mode in [\"FT\",\"FT-L\",\"ROME\",\"KE\",\"MEND\"]:\n",
    "        # self.weights = ...\n",
    "\n",
    "    def update_edit_mode(self, hparams):\n",
    "        self.params = hparams\n",
    "        self.preprompt = \"\"\n",
    "\n",
    "    \n",
    "    def edit(self, request):\n",
    "        \n",
    "        if self.params[\"mode\"] == \"ICE\":\n",
    "            self.preprompt = request[\"preprompt\"]\n",
    "        \n",
    "    \n",
    "    def restore(self):\n",
    "        self.preprompt = \"\"\n",
    "        # self.model ... ## restore weights\n",
    "\n",
    "    \n",
    "    def generate_text(self, texts, **kwargs):\n",
    "        \n",
    "        if type(texts) != list:\n",
    "            texts = [texts]\n",
    "        \n",
    "        texts = [self.preprompt + t for t in texts]\n",
    "\n",
    "        tokenizer = self.tok\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(**encoding, **kwargs) # \n",
    "\n",
    "            generated_texts = tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "        return(generated_texts)\n",
    "\n",
    "    \n",
    "    def token_logit(self, texts, token, start_ind = None):\n",
    "        \n",
    "        texts = self.preprompt + texts\n",
    "    \n",
    "        tokenizer = self.tok \n",
    "        model = self.model\n",
    "        encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_out = model(encoding[\"input_ids\"])\n",
    "            logits = model_out.logits\n",
    "            logprobs = F.log_softmax(logits, -1)\n",
    "\n",
    "        token_id = encode_token(token, tokenizer)\n",
    "        start_ind = -len(token_id)-1 if not start_ind else start_ind\n",
    "        \n",
    "        l = logprobs[:, start_ind:-1, token_id]\n",
    "        if len(l.squeeze().shape) == 0:\n",
    "            return(l.squeeze())\n",
    "        else:\n",
    "            return(l.squeeze().diag().sum())\n",
    "        \n",
    "\n",
    "    def choose(self, prompt, choices):\n",
    "        prompts = [prompt + pad_token(c) for c in choices]\n",
    "        logits = [self.token_logit(prompts[i], choices[i]) for i in range(len(choices))]\n",
    "        return(logits.index(max(logits)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = EditedModel(model, tokenizer)\n",
    "m = EditedModel(model, tokenizer, {\"mode\":\"ICE\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# m.edit({\"preprompt\": \"Imagine that a terrier is a kind of horse. In this case: \"})\n",
    "print(m.choose(\"A terrier is something people like to\", [\"pet\", \"eat\", \"ride\"]))\n",
    "m.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0",
   "language": "python",
   "name": "pytorch-gpu-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
